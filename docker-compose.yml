networks:
  lakehouse:
    name: lakehouse
    driver: bridge

services:
  minio:
    image: minio/minio:RELEASE.2024-11-07T00-52-20Z
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: minio server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - lakehouse
    cpus: '1'
    mem_limit: 1G

  iceberg-db:
    image: postgres:15
    container_name: iceberg-db
    ports:
      - "5432:5432"
    volumes:
      - iceberg-db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: iceberg
      POSTGRES_PASSWORD: icebergpassword123
      POSTGRES_DB: iceberg
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U iceberg"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse
    cpus: '0.5'
    mem_limit: 512M

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    entrypoint: >
      /bin/sh -c "
      mc alias set minio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD} --api S3v4;
      mc mb --ignore-existing minio/lakehouse;
      echo 'âœ“ MinIO bucket lakehouse created';
      "
    networks:
      - lakehouse

  iceberg-rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://lakehouse/warehouse
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
    depends_on:
      minio:
        condition: service_healthy
      iceberg-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1/config"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - lakehouse
    cpus: '1'
    mem_limit: 1G

  trino:
    image: trinodb/trino:445
    container_name: trino
    ports:
      - "8082:8080"
    volumes:
      - ./trino/etc:/etc/trino
      - ./trino/catalog:/etc/trino/catalog
    depends_on:
      iceberg-rest:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    networks:
      - lakehouse
    cpus: '2'
    mem_limit: 4G

  spark:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: spark
    ports:
      - "8888:8888"
      - "8084:8080"
      - "10000:10000"
      - "10001:10001"
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/notebooks:/opt/spark/notebooks
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_S3_ENDPOINT: http://minio:9000
      ICEBERG_REST_URI: http://iceberg-rest:8181
    depends_on:
      iceberg-rest:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s -o /dev/null -w '%{http_code}' http://localhost:8888/ | grep -q '200\\|302'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    networks:
      - lakehouse
    cpus: '2'
    mem_limit: 6G

  airflow-db:
    image: postgres:15
    container_name: airflow-db
    ports:
      - "5435:5432"
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflowpassword123
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse
    cpus: '0.5'
    mem_limit: 512M

  airflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: airflow
    ports:
      - "8083:8080"
      - "8793:8793"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./dbt:/opt/dbt
      - ./scripts:/opt/scripts
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW_UID: 50000
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflowpassword123@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    depends_on:
      airflow-db:
        condition: service_healthy
      trino:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - lakehouse
    command: bash -c "airflow standalone"
    cpus: '1'
    mem_limit: 2G

volumes:
  minio-data:
  iceberg-db-data:
  airflow-db-data:
